{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we walk through all the necessary components of running experiments on LIBERO, and some common usage such as defining your own algorithm and policy architectures in the codebase.\n",
    "\n",
    "1. Dataset preparation for your algorithms\n",
    "2. Write your own algorithm\n",
    "    - Subclassing from `Sequential` base class\n",
    "3. Write your own model\n",
    "4. Write your training loop\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T17:51:07.521945Z",
     "start_time": "2024-08-23T17:51:07.388646Z"
    }
   },
   "source": [
    "from hydra import compose, initialize\n",
    "\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "import hydra\n",
    "import pprint\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.lifelong.datasets import (GroupedTaskDataset, SequenceVLDataset, get_dataset)\n",
    "from libero.lifelong.utils import (get_task_embs, safe_device, create_experiment_dir)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "### load the default hydra config\n",
    "initialize(config_path=\"../libero/configs\")\n",
    "hydra_cfg = compose(config_name=\"config\")\n",
    "yaml_config = OmegaConf.to_yaml(hydra_cfg)\n",
    "cfg = EasyDict(yaml.safe_load(yaml_config))\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(cfg.policy)\n",
    "\n",
    "# prepare lifelong learning\n",
    "cfg.folder = get_libero_path(\"datasets\")\n",
    "cfg.bddl_folder = get_libero_path(\"bddl_files\")\n",
    "cfg.init_states_folder = get_libero_path(\"init_states\")\n",
    "cfg.eval.num_procs = 1\n",
    "cfg.eval.n_eval = 1\n",
    "\n",
    "cfg.train.n_epochs = 1#25\n",
    "\n",
    "pp.pprint(f\"Note that the number of epochs used in this example is intentionally reduced to 5.\")\n",
    "task_order = cfg.data.task_order_index # can be from {0 .. 21}, default to 0, which is [task 0, 1, 2 ...]\n",
    "cfg.benchmark_name = 'miniset_1'#\"libero_object\" # can be from {\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_10\"}\n",
    "benchmark = get_benchmark(cfg.benchmark_name)(task_order)\n",
    "\n",
    "# prepare datasets from the benchmark\n",
    "datasets = []\n",
    "descriptions = []\n",
    "shape_meta = None\n",
    "n_tasks = benchmark.n_tasks\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    # currently we assume tasks from same benchmark have the same shape_meta\n",
    "    task_i_dataset, shape_meta = get_dataset(\n",
    "            dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)),\n",
    "            obs_modality=cfg.data.obs.modality,\n",
    "            initialize_obs_utils=(i==0),\n",
    "            seq_len=cfg.data.seq_len,\n",
    "    )\n",
    "    # add language to the vision dataset, hence we call vl_dataset\n",
    "    descriptions.append(benchmark.get_task(i).language)\n",
    "    datasets.append(task_i_dataset)\n",
    "\n",
    "task_embs = get_task_embs(cfg, descriptions)\n",
    "benchmark.set_task_embs(task_embs)\n",
    "\n",
    "datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)]\n",
    "n_demos = [data.n_demos for data in datasets]\n",
    "n_sequences = [data.total_num_sequences for data in datasets]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'color_aug': { 'network': 'BatchWiseImgColorJitterAug',\n",
      "                 'network_kwargs': { 'brightness': 0.3,\n",
      "                                     'contrast': 0.3,\n",
      "                                     'epsilon': 0.1,\n",
      "                                     'hue': 0.3,\n",
      "                                     'input_shape': None,\n",
      "                                     'saturation': 0.3}},\n",
      "  'embed_size': 64,\n",
      "  'extra_hidden_size': 128,\n",
      "  'extra_num_layers': 0,\n",
      "  'image_encoder': { 'network': 'ResnetEncoder',\n",
      "                     'network_kwargs': { 'freeze': False,\n",
      "                                         'language_fusion': 'film',\n",
      "                                         'no_stride': False,\n",
      "                                         'pretrained': False,\n",
      "                                         'remove_layer_num': 4}},\n",
      "  'language_encoder': { 'network': 'MLPEncoder',\n",
      "                        'network_kwargs': { 'hidden_size': 128,\n",
      "                                            'input_size': 768,\n",
      "                                            'num_layers': 1,\n",
      "                                            'output_size': 128}},\n",
      "  'policy_head': { 'loss_kwargs': {'loss_coef': 1.0},\n",
      "                   'network': 'GMMHead',\n",
      "                   'network_kwargs': { 'activation': 'softplus',\n",
      "                                       'hidden_size': 1024,\n",
      "                                       'low_eval_noise': False,\n",
      "                                       'min_std': 0.0001,\n",
      "                                       'num_layers': 2,\n",
      "                                       'num_modes': 5}},\n",
      "  'policy_type': 'BCTransformerPolicy',\n",
      "  'temporal_position_encoding': { 'network': 'SinusoidalPositionEncoding',\n",
      "                                  'network_kwargs': { 'factor_ratio': None,\n",
      "                                                      'input_size': None,\n",
      "                                                      'inv_freq_factor': 10}},\n",
      "  'transformer_dropout': 0.1,\n",
      "  'transformer_head_output_size': 64,\n",
      "  'transformer_input_size': None,\n",
      "  'transformer_max_seq_len': 10,\n",
      "  'transformer_mlp_hidden_size': 256,\n",
      "  'transformer_num_heads': 6,\n",
      "  'transformer_num_layers': 4,\n",
      "  'translation_aug': { 'network': 'TranslationAug',\n",
      "                       'network_kwargs': { 'input_shape': None,\n",
      "                                           'translation': 8}}}\n",
      "('Note that the number of epochs used in this example is intentionally reduced '\n",
      " 'to 5.')\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21129/2878958379.py:18: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../libero/configs\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m task_order \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mtask_order_index \u001B[38;5;66;03m# can be from {0 .. 21}, default to 0, which is [task 0, 1, 2 ...]\u001B[39;00m\n\u001B[1;32m     37\u001B[0m cfg\u001B[38;5;241m.\u001B[39mbenchmark_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminiset_3\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;66;03m#\"libero_object\" # can be from {\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_10\"}\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m benchmark \u001B[38;5;241m=\u001B[39m \u001B[43mget_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbenchmark_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask_order\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# prepare datasets from the benchmark\u001B[39;00m\n\u001B[1;32m     41\u001B[0m datasets \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Documents/LIBERO Work/LIBERO/libero/libero/benchmark/__init__.py:228\u001B[0m, in \u001B[0;36mminiset_3.__init__\u001B[0;34m(self, task_order_index)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(task_order_index\u001B[38;5;241m=\u001B[39mtask_order_index)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminiset_3\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 228\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LIBERO Work/LIBERO/libero/libero/benchmark/__init__.py:122\u001B[0m, in \u001B[0;36mBenchmark._make_benchmark\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[info] using task orders \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtask_orders[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask_order_index]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtasks \u001B[38;5;241m=\u001B[39m [tasks[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m task_orders[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask_order_index]]\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtasks)\n",
      "File \u001B[0;32m~/Documents/LIBERO Work/LIBERO/libero/libero/benchmark/__init__.py:122\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[info] using task orders \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtask_orders[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask_order_index]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtasks \u001B[38;5;241m=\u001B[39m [\u001B[43mtasks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m task_orders[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask_order_index]]\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtasks)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write your own policy architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from libero.lifelong.models.modules.rgb_modules import *\n",
    "from libero.lifelong.models.modules.language_modules import *\n",
    "from libero.lifelong.models.base_policy import BasePolicy\n",
    "from libero.lifelong.models.policy_head import *\n",
    "from libero.lifelong.models.modules.transformer_modules import *\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A model handling extra input modalities besides images at time t.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "## Just to be quick while working, clear the policies here\n",
    "from libero.lifelong.models.base_policy import REGISTERED_POLICIES\n",
    "REGISTERED_POLICIES.clear()\n",
    "\n",
    "class ExtraModalityTokens(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_joint=False,\n",
    "        use_gripper=False,\n",
    "        use_ee=False,\n",
    "        extra_num_layers=0,\n",
    "        extra_hidden_size=32,\n",
    "        extra_embedding_size=16,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This is a class that maps all extra modality inputs into tokens of the same size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_joint = use_joint\n",
    "        self.use_gripper = use_gripper\n",
    "        self.use_ee = use_ee\n",
    "        self.extra_embedding_size = extra_embedding_size\n",
    "\n",
    "        joint_states_dim = 7\n",
    "        gripper_states_dim = 2\n",
    "        ee_dim = 3\n",
    "\n",
    "        self.num_extra = int(use_joint) + int(use_gripper) + int(use_ee)\n",
    "\n",
    "        extra_low_level_feature_dim = (\n",
    "            int(use_joint) * joint_states_dim\n",
    "            + int(use_gripper) * gripper_states_dim\n",
    "            + int(use_ee) * ee_dim\n",
    "        )\n",
    "\n",
    "        assert extra_low_level_feature_dim > 0, \"[error] no extra information\"\n",
    "\n",
    "        self.extra_encoders = {}\n",
    "\n",
    "        def generate_proprio_mlp_fn(modality_name, extra_low_level_feature_dim):\n",
    "            assert extra_low_level_feature_dim > 0  # we indeed have extra information\n",
    "            if extra_num_layers > 0:\n",
    "                layers = [nn.Linear(extra_low_level_feature_dim, extra_hidden_size)]\n",
    "                for i in range(1, extra_num_layers):\n",
    "                    layers += [\n",
    "                        nn.Linear(extra_hidden_size, extra_hidden_size),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                    ]\n",
    "                layers += [nn.Linear(extra_hidden_size, extra_embedding_size)]\n",
    "            else:\n",
    "                layers = [nn.Linear(extra_low_level_feature_dim, extra_embedding_size)]\n",
    "\n",
    "            self.proprio_mlp = nn.Sequential(*layers)\n",
    "            self.extra_encoders[modality_name] = {\"encoder\": self.proprio_mlp}\n",
    "\n",
    "        for (proprio_dim, use_modality, modality_name) in [\n",
    "            (joint_states_dim, self.use_joint, \"joint_states\"),\n",
    "            (gripper_states_dim, self.use_gripper, \"gripper_states\"),\n",
    "            (ee_dim, self.use_ee, \"ee_states\"),\n",
    "        ]:\n",
    "\n",
    "            if use_modality:\n",
    "                generate_proprio_mlp_fn(modality_name, proprio_dim)\n",
    "\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [x[\"encoder\"] for x in self.extra_encoders.values()]\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_dict):\n",
    "        \"\"\"\n",
    "        obs_dict: {\n",
    "            (optional) joint_stats: (B, T, 7),\n",
    "            (optional) gripper_states: (B, T, 2),\n",
    "            (optional) ee: (B, T, 3)\n",
    "        }\n",
    "        map above to a latent vector of shape (B, T, H)\n",
    "        \"\"\"\n",
    "        tensor_list = []\n",
    "\n",
    "        for (use_modality, modality_name) in [\n",
    "            (self.use_joint, \"joint_states\"),\n",
    "            (self.use_gripper, \"gripper_states\"),\n",
    "            (self.use_ee, \"ee_states\"),\n",
    "        ]:\n",
    "\n",
    "            if use_modality:\n",
    "                tensor_list.append(\n",
    "                    self.extra_encoders[modality_name][\"encoder\"](\n",
    "                        obs_dict[modality_name]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        x = torch.stack(tensor_list, dim=-2)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A Transformer policy\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class MyTransformerPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Input: (o_{t-H}, ... , o_t)\n",
    "    Output: a_t or distribution of a_t\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, shape_meta):\n",
    "        super().__init__(cfg, shape_meta)\n",
    "        policy_cfg = cfg.policy\n",
    "\n",
    "        ### 1. encode image\n",
    "        embed_size = policy_cfg.embed_size\n",
    "        transformer_input_sizes = []\n",
    "        self.image_encoders = {}\n",
    "        for name in shape_meta[\"all_shapes\"].keys():\n",
    "            if \"rgb\" in name or \"depth\" in name:\n",
    "                kwargs = policy_cfg.image_encoder.network_kwargs\n",
    "                kwargs.input_shape = shape_meta[\"all_shapes\"][name]\n",
    "                kwargs.output_size = embed_size\n",
    "                kwargs.language_dim = (\n",
    "                    policy_cfg.language_encoder.network_kwargs.input_size\n",
    "                )\n",
    "                self.image_encoders[name] = {\n",
    "                    \"input_shape\": shape_meta[\"all_shapes\"][name],\n",
    "                    \"encoder\": eval(policy_cfg.image_encoder.network)(**kwargs),\n",
    "                }\n",
    "\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [x[\"encoder\"] for x in self.image_encoders.values()]\n",
    "        )\n",
    "\n",
    "        ### 2. encode language\n",
    "        policy_cfg.language_encoder.network_kwargs.output_size = embed_size\n",
    "        self.language_encoder = eval(policy_cfg.language_encoder.network)(\n",
    "            **policy_cfg.language_encoder.network_kwargs\n",
    "        )\n",
    "\n",
    "        ### 3. encode extra information (e.g. gripper, joint_state)\n",
    "        self.extra_encoder = ExtraModalityTokens(\n",
    "            use_joint=cfg.data.use_joint,\n",
    "            use_gripper=cfg.data.use_gripper,\n",
    "            use_ee=cfg.data.use_ee,\n",
    "            extra_num_layers=policy_cfg.extra_num_layers,\n",
    "            extra_hidden_size=policy_cfg.extra_hidden_size,\n",
    "            extra_embedding_size=embed_size,\n",
    "        )\n",
    "\n",
    "        ### 4. define temporal transformer\n",
    "        policy_cfg.temporal_position_encoding.network_kwargs.input_size = embed_size\n",
    "        self.temporal_position_encoding_fn = eval(\n",
    "            policy_cfg.temporal_position_encoding.network\n",
    "        )(**policy_cfg.temporal_position_encoding.network_kwargs)\n",
    "\n",
    "        self.temporal_transformer = TransformerDecoder(\n",
    "            input_size=embed_size,\n",
    "            num_layers=policy_cfg.transformer_num_layers,\n",
    "            num_heads=policy_cfg.transformer_num_heads,\n",
    "            head_output_size=policy_cfg.transformer_head_output_size,\n",
    "            mlp_hidden_size=policy_cfg.transformer_mlp_hidden_size,\n",
    "            dropout=policy_cfg.transformer_dropout,\n",
    "        )\n",
    "\n",
    "        policy_head_kwargs = policy_cfg.policy_head.network_kwargs\n",
    "        policy_head_kwargs.input_size = embed_size\n",
    "        policy_head_kwargs.output_size = shape_meta[\"ac_dim\"]\n",
    "\n",
    "        self.policy_head = eval(policy_cfg.policy_head.network)(\n",
    "            **policy_cfg.policy_head.loss_kwargs,\n",
    "            **policy_cfg.policy_head.network_kwargs\n",
    "        )\n",
    "\n",
    "        self.latent_queue = []\n",
    "        self.max_seq_len = policy_cfg.transformer_max_seq_len\n",
    "\n",
    "    def temporal_encode(self, x):\n",
    "        pos_emb = self.temporal_position_encoding_fn(x)\n",
    "        x = x + pos_emb.unsqueeze(1)  # (B, T, num_modality, E)\n",
    "        sh = x.shape\n",
    "        self.temporal_transformer.compute_mask(x.shape)\n",
    "\n",
    "        x = TensorUtils.join_dimensions(x, 1, 2)  # (B, T*num_modality, E)\n",
    "        x = self.temporal_transformer(x)\n",
    "        x = x.reshape(*sh)\n",
    "        return x[:, :, 0]  # (B, T, E)\n",
    "\n",
    "    def spatial_encode(self, data):\n",
    "        # 1. encode extra\n",
    "        extra = self.extra_encoder(data[\"obs\"])  # (B, T, num_extra, E)\n",
    "\n",
    "        # 2. encode language, treat it as action token\n",
    "        B, T = extra.shape[:2]\n",
    "        text_encoded = self.language_encoder(data)  # (B, E)\n",
    "        text_encoded = text_encoded.view(B, 1, 1, -1).expand(\n",
    "            -1, T, -1, -1\n",
    "        )  # (B, T, 1, E)\n",
    "        encoded = [text_encoded, extra]\n",
    "\n",
    "        # 3. encode image\n",
    "        for img_name in self.image_encoders.keys():\n",
    "            x = data[\"obs\"][img_name]\n",
    "            B, T, C, H, W = x.shape\n",
    "            img_encoded = self.image_encoders[img_name][\"encoder\"](\n",
    "                x.reshape(B * T, C, H, W),\n",
    "                langs=data[\"task_emb\"]\n",
    "                .reshape(B, 1, -1)\n",
    "                .repeat(1, T, 1)\n",
    "                .reshape(B * T, -1),\n",
    "            ).view(B, T, 1, -1)\n",
    "            encoded.append(img_encoded)\n",
    "        encoded = torch.cat(encoded, -2)  # (B, T, num_modalities, E)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.spatial_encode(data)\n",
    "        x = self.temporal_encode(x)\n",
    "        dist = self.policy_head(x)\n",
    "        return dist\n",
    "\n",
    "    def get_action(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            data = self.preprocess_input(data, train_mode=False)\n",
    "            x = self.spatial_encode(data)\n",
    "            self.latent_queue.append(x)\n",
    "            if len(self.latent_queue) > self.max_seq_len:\n",
    "                self.latent_queue.pop(0)\n",
    "            x = torch.cat(self.latent_queue, dim=1)  # (B, T, H_all)\n",
    "            x = self.temporal_encode(x)\n",
    "            dist = self.policy_head(x[:, -1])\n",
    "        action = dist.sample().detach().cpu()\n",
    "        return action.view(action.shape[0], -1).numpy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.latent_queue = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write your own lifelong learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from libero.lifelong.algos.base import Sequential\n",
    "\n",
    "### All lifelong learning algorithm should inherit the Sequential algorithm super class\n",
    "\n",
    "## Just to be quick while working, clear the algos here\n",
    "from libero.lifelong.algos.base import REGISTERED_ALGOS\n",
    "REGISTERED_ALGOS.clear()\n",
    "\n",
    "class MyLifelongAlgo(Sequential):\n",
    "    \"\"\"\n",
    "    The experience replay policy.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_tasks,\n",
    "                 cfg,\n",
    "                 **policy_kwargs):\n",
    "        super().__init__(n_tasks=n_tasks, cfg=cfg, **policy_kwargs)\n",
    "        # define the learning policy\n",
    "        self.datasets = []\n",
    "        self.policy = eval(cfg.policy.policy_type)(cfg, cfg.shape_meta)\n",
    "\n",
    "    def start_task(self, task):\n",
    "        # what to do at the beginning of a new task\n",
    "        super().start_task(task)\n",
    "\n",
    "    def end_task(self, dataset, task_id, benchmark):\n",
    "        # what to do when finish learning a new task\n",
    "        self.datasets.append(dataset)\n",
    "\n",
    "    def observe(self, data):\n",
    "        # how the algorithm observes a data and returns a loss to be optimized\n",
    "        loss = super().observe(data)\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write your training script"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cfg.policy.policy_type = \"MyTransformerPolicy\"\n",
    "cfg.lifelong.algo = \"MyLifelongAlgo\"\n",
    "\n",
    "create_experiment_dir(cfg)\n",
    "cfg.shape_meta = shape_meta\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from libero.lifelong.metric import evaluate_loss, evaluate_success\n",
    "\n",
    "print(\"experiment directory is: \", cfg.experiment_dir)\n",
    "algo = safe_device(MyLifelongAlgo(n_tasks, cfg), cfg.device)\n",
    "\n",
    "result_summary = {\n",
    "    'L_conf_mat': np.zeros((n_tasks, n_tasks)),   # loss confusion matrix\n",
    "    'S_conf_mat': np.zeros((n_tasks, n_tasks)),   # success confusion matrix\n",
    "    'L_fwd'     : np.zeros((n_tasks,)),           # loss AUC, how fast the agent learns\n",
    "    'S_fwd'     : np.zeros((n_tasks,)),           # success AUC, how fast the agent succeeds\n",
    "}\n",
    "\n",
    "gsz = cfg.data.task_group_size\n",
    "\n",
    "if (cfg.train.n_epochs < 50):\n",
    "    print(\"NOTE: the number of epochs used in this example is intentionally reduced to 30 for simplicity.\")\n",
    "if (cfg.eval.n_eval < 20):\n",
    "    print(\"NOTE: the number of evaluation episodes used in this example is intentionally reduced to 5 for simplicity.\")\n",
    "\n",
    "for i in trange(n_tasks):\n",
    "    algo.train()\n",
    "    s_fwd, l_fwd = algo.learn_one_task(datasets[i], i, benchmark, result_summary)\n",
    "    # s_fwd is success rate AUC, when the agent learns the {0, e, 2e, ...} epochs\n",
    "    # l_fwd is BC loss AUC, similar to s_fwd\n",
    "    result_summary[\"S_fwd\"][i] = s_fwd\n",
    "    result_summary[\"L_fwd\"][i] = l_fwd\n",
    "\n",
    "    if cfg.eval.eval:\n",
    "        algo.eval()\n",
    "        # we only evaluate on the past tasks: 0 .. i\n",
    "        L = evaluate_loss(cfg, algo, benchmark, datasets[:i+1]) # (i+1,)\n",
    "        S = evaluate_success(cfg, algo, benchmark, list(range((i+1)*gsz))) # (i+1,)\n",
    "        result_summary[\"L_conf_mat\"][i][:i+1] = L\n",
    "        result_summary[\"S_conf_mat\"][i][:i+1] = S\n",
    "\n",
    "        torch.save(result_summary, os.path.join(cfg.experiment_dir, f'result.pt'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result_summary = torch.load(os.path.join(cfg.experiment_dir, f'result.pt'))\n",
    "print(result_summary[\"S_conf_mat\"])\n",
    "print(result_summary[\"S_fwd\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compute FWT, BWT, and AUC of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "benchmark_map = {\n",
    "    \"libero_10\"     : \"LIBERO_10\",\n",
    "    \"libero_90\"     : \"LIBERO_90\",\n",
    "    \"libero_spatial\": \"LIBERO_SPATIAL\",\n",
    "    \"libero_object\" : \"LIBERO_OBJECT\",\n",
    "    \"libero_goal\"   : \"LIBERO_GOAL\",\n",
    "}\n",
    "\n",
    "algo_map = {\n",
    "    \"base\"     : \"Sequential\",\n",
    "    \"er\"       : \"ER\",\n",
    "    \"ewc\"      : \"EWC\",\n",
    "    \"packnet\"  : \"PackNet\",\n",
    "    \"multitask\": \"Multitask\",\n",
    "    \"custom_algo\"   : \"MyLifelongAlgo\",\n",
    "}\n",
    "\n",
    "policy_map = {\n",
    "    \"bc_rnn_policy\"        : \"BCRNNPolicy\",\n",
    "    \"bc_transformer_policy\": \"BCTransformerPolicy\",\n",
    "    \"bc_vilt_policy\"       : \"BCViLTPolicy\",\n",
    "    \"custom_policy\"        : \"MyTransformerPolicy\",\n",
    "}\n",
    "\n",
    "seeds = [10000]\n",
    "N_SEEDS = len(seeds)\n",
    "N_TASKS = 10\n",
    "\n",
    "def get_auc(experiment_dir, bench, algo, policy):\n",
    "    N_EP = cfg.train.n_epochs // cfg.eval.eval_every + 1\n",
    "    fwds = np.zeros((N_TASKS, N_EP, N_SEEDS))\n",
    "\n",
    "    for task in range(N_TASKS):\n",
    "        counter = 0\n",
    "        for k, seed in enumerate(seeds):\n",
    "            name = f\"{experiment_dir}/task{task}_auc.log\"\n",
    "            try:\n",
    "                succ = torch.load(name)[\"success\"] # (n_epochs)\n",
    "                idx = succ.argmax()\n",
    "                succ[idx:] = succ[idx]\n",
    "                fwds[task, :, k] = succ\n",
    "            except:\n",
    "                print(\"Some errors when loading results\")\n",
    "                continue\n",
    "    return fwds\n",
    "\n",
    "def compute_metric(res):\n",
    "    mat, fwts  = res # fwds: (num_tasks, num_save_intervals, num_seeds)\n",
    "    num_tasks, num_seeds = mat.shape[1:]\n",
    "    ret = {}\n",
    "\n",
    "    # compute fwt\n",
    "    fwt = fwts.mean(axis=(0,1))\n",
    "    ret[\"fwt\"] = fwt\n",
    "    # compute bwt\n",
    "    bwts = []\n",
    "    aucs = []\n",
    "    for seed in range(num_seeds):\n",
    "        bwt = 0.0\n",
    "        auc = 0.0\n",
    "        for k in range(num_tasks):\n",
    "            bwt_k = 0.0\n",
    "            auc_k = 0.0\n",
    "            for tau in range(k+1, num_tasks):\n",
    "                bwt_k += mat[k,k,seed] - mat[tau,k,seed]\n",
    "                auc_k += mat[tau,k,seed]\n",
    "            if k + 1 < num_tasks:\n",
    "                bwt_k /= (num_tasks - k - 1)\n",
    "            auc_k = (auc_k + fwts[k,:,seed].mean()) / (num_tasks - k)\n",
    "\n",
    "            bwt += bwt_k\n",
    "            auc += auc_k\n",
    "        bwts.append(bwt / num_tasks)\n",
    "        aucs.append(auc / num_tasks)\n",
    "    bwts = np.array(bwts)\n",
    "    aucs = np.array(aucs)\n",
    "    ret[\"bwt\"] = bwts\n",
    "    ret[\"auc\"] = aucs\n",
    "    return ret"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "experiment_dir = \"experiments\"\n",
    "benchmark_name = \"libero_object\"\n",
    "algo_name = \"custom_algo\"\n",
    "policy_name = \"custom_policy\"\n",
    "\n",
    "fwds = get_auc(cfg.experiment_dir, benchmark_name, algo_name, policy_name)\n",
    "\n",
    "conf_mat = result_summary[\"S_conf_mat\"][..., np.newaxis]\n",
    "\n",
    "metric = compute_metric((conf_mat, fwds))\n",
    "print(metric)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize policy rollouts\n",
    "\n",
    " This is an example of how to use the trained model to do inference. We will take the policy from training on the first task as an example. More concrete example, please see `evaluate_one_task_success` in the file `lifelong/lifelong/metric.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio\n",
    "\n",
    "from libero.libero.envs import OffScreenRenderEnv, DummyVectorEnv\n",
    "from libero.lifelong.metric import raw_obs_to_tensor_obs\n",
    "\n",
    "# You can turn on subprocess\n",
    "env_num = 1\n",
    "action_dim = 7\n",
    "\n",
    "\n",
    "# If it's packnet, the weights need to be processed first\n",
    "task_id = 0\n",
    "task = benchmark.get_task(task_id)\n",
    "task_emb = benchmark.get_task_emb(task_id)\n",
    "\n",
    "if cfg.lifelong.algo == \"PackNet\":\n",
    "    algo = algo.get_eval_algo(task_id)\n",
    "\n",
    "algo.eval()\n",
    "env_args = {\n",
    "    \"bddl_file_name\": os.path.join(\n",
    "        cfg.bddl_folder, task.problem_folder, task.bddl_file\n",
    "    ),\n",
    "    \"camera_heights\": cfg.data.img_h,\n",
    "    \"camera_widths\": cfg.data.img_w,\n",
    "}\n",
    "\n",
    "env = DummyVectorEnv(\n",
    "            [lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)]\n",
    ")\n",
    "\n",
    "init_states_path = os.path.join(\n",
    "    cfg.init_states_folder, task.problem_folder, task.init_states_file\n",
    ")\n",
    "init_states = torch.load(init_states_path)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "init_state = init_states[0:1]\n",
    "dones = [False]\n",
    "\n",
    "algo.reset()\n",
    "\n",
    "obs = env.set_init_state(init_state)\n",
    "\n",
    "\n",
    "# Make sure the gripepr is open to make it consistent with the provided demos.\n",
    "dummy_actions = np.zeros((env_num, action_dim))\n",
    "for _ in range(5):\n",
    "    obs, _, _, _ = env.step(dummy_actions)\n",
    "\n",
    "steps = 0\n",
    "\n",
    "obs_tensors = [[]] * env_num\n",
    "while steps < cfg.eval.max_steps:\n",
    "    steps += 1\n",
    "    data = raw_obs_to_tensor_obs(obs, task_emb, cfg)\n",
    "    action = algo.policy.get_action(data)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    for k in range(env_num):\n",
    "        dones[k] = dones[k] or done[k]\n",
    "        obs_tensors[k].append(obs[k][\"agentview_image\"])\n",
    "    if all(dones):\n",
    "        break\n",
    "\n",
    "# visualize video\n",
    "# obs_tensor: (env_num, T, H, W, C)\n",
    "\n",
    "images = [img[::-1] for img in obs_tensors[0]]\n",
    "fps = 30\n",
    "writer  = imageio.get_writer('tmp_video.mp4', fps=fps)\n",
    "for image in images:\n",
    "    writer.append_data(image)\n",
    "writer.close()\n",
    "\n",
    "video_data = open(\"tmp_video.mp4\", \"rb\").read()\n",
    "video_tag = f'<video controls alt=\"test\" src=\"data:video/mp4;base64,{b64encode(video_data).decode()}\">'\n",
    "HTML(data=video_tag)\n",
    "\n",
    "import os\n",
    "video_path = os.path.abspath(\"tmp_video.mp4\")\n",
    "print(video_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
